Build a full-stack real-time AI assistant that:
â€¢	Listens to live voice input via Web Speech API
â€¢	Accepts manual text context input (like resume or notes)
â€¢	Uses context + conversation to generate instant, streaming text answers
â€¢	Maintains short-term memory (last 5â€“10 exchanges)
â€¢	Uses Groq API (Mixtral) or Together AI (Mistral) for fast LLM responses
â€¢	Uses WebSockets or SSE to stream responses to frontend in real time
ğŸ§  Must-Have Functional Requirements:
Frontend (React + TailwindCSS):
â€¢	ContextInput.jsx: paste context text
â€¢	MicInput.jsx: mic button for capturing voice using Web Speech API
â€¢	ChatWindow.jsx: shows transcribed questions + AI answers in real time
â€¢	WebSocket or EventSource connection to stream responses
Backend (Node.js + Express):
â€¢	POST /context: store user context
â€¢	POST /chat: receive question, generate AI answer using context + memory
â€¢	GET /stream: send AI reply via WebSocket or SSE
â€¢	In-memory storage of context and short-term chat history
ğŸ“ Project Structure (Must Follow):
ai-interview-assistant/
â”œâ”€â”€ client/             # React Frontend
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.jsx
â”‚       â”œâ”€â”€ index.js
â”‚       â”œâ”€â”€ components/
â”‚           â”œâ”€â”€ MicInput.jsx
â”‚           â”œâ”€â”€ ContextInput.jsx
â”‚           â””â”€â”€ ChatWindow.jsx
â”œâ”€â”€ server/             # Node.js Backend
â”‚   â”œâ”€â”€ index.js
â”‚   â”œâ”€â”€ routes/
â”‚       â”œâ”€â”€ context.js
â”‚       â”œâ”€â”€ chat.js
â”‚   â”œâ”€â”€ services/
â”‚       â”œâ”€â”€ groqService.js
â”‚       â””â”€â”€ memory.js
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
â””â”€â”€ package.json        # With workspaces if monorepo
âš™ï¸ Local & Replit Compatibility:
â€¢	Include README.md with clear local run instructions:
1.	npm install (root)
2.	cd client && npm install && npm run dev
3.	cd server && npm install && npm run dev
â€¢	Use .env.example file for local API keys (GROQ_API_KEY, etc.)
â€¢	Set CORS and ports properly (client on 5173, server on 5000 or similar)
ğŸ›¡ï¸ Security:
â€¢	Do not store context or history permanently
â€¢	Keep data in-memory per session
â€¢	Secure API keys using .env and never expose to frontend
ğŸ¯ Performance:
â€¢	Target <1 sec total latency
â€¢	Stream answers using res.write() or WebSocket chunking
